getwd()
setwd(getwd())
read.csv("./drug200.csv")
df <-read.csv("./drug200.csv")
head(df)
str(df)
df[] <- lapply(df, function(x) {
if (is.character(x) || is.factor(x)) {
as.numeric(as.factor(x))
} else {
x
}
})
# View transformed data
head(df)
# View transformed data
head(df, 10)
summary(df)
setwd(getwd())
df <-read.csv("./drug200.csv")
head(df)
str(df)
summary(df)
summary(df)
df[] <- lapply(df, function(x) {
if (is.character(x) || is.factor(x)) {
as.numeric(as.factor(x))
} else {
x
}
})
# View transformed data
head(df, 10)
summary(df)
library(rpart)
tree_model <- rpart(Drug ~ ., data = df, method = "class")
library(rpart.plot)
prp(tree_model)
prp(tree_model)
best_cp <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]), "CP"]
pruned_tree <- prune(tree_model, cp = best_cp)
prp(pruned_tree)
best_cp <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]), "CP"]
pruned_tree <- prune(tree_model, cp = best_cp)
prp(pruned_tree)
predictions <- predict(pruned_tree, df, type = "class")
misclassification_rate <- mean(predictions != df$Drug)
accuracy <- 1 - misclassification_rate
print(accuracy)
library(randomForest)
rf_model <- randomForest(Drug ~ ., data = df, ntree = 500)
print(rf_model)
rf_model_tuned <- randomForest(Drug ~ ., data = df, ntree = 500, mtry = 3)
print(rf_model_tuned)
library(caret)
tune_grid <- expand.grid(mtry = c(2, 3, 4))
control <- trainControl(method = "cv", number = 5)
rf_tuned <- train(Drug ~ ., data = df, method = "rf", tuneGrid = tune_grid, trControl = control)
print(rf_tuned$results)
best_accuracy <- max(rf_tuned$results$Accuracy)
print(best_accuracy)
tune_grid <- expand.grid(mtry = c(2, 3, 4))
control <- trainControl(method = "cv", number = 5)
rf_tuned <- train(Drug ~ ., data = df, method = "rf", tuneGrid = tune_grid, trControl = control)
print(rf_tuned$results)
best_accuracy <- max(rf_tuned$results$Accuracy)
print(best_accuracy)
library(ipred)
bagging_model <- bagging(Drug ~ ., data = df, nbagg = 100)
bagging_predictions <- predict(bagging_model, df)
bagging_accuracy <- mean(bagging_predictions == df$Drug)
print(bagging_accuracy)
# 1) Import the designated data file
df <- read.csv("drug200.csv")
# 2) Data cleaning and pre-processing
# Check structure and summary
str(df)
summary(df)
# Handle missing values if any
df <- na.omit(df)
# 4) Convert categorical inputs
# Convert categorical variables to factors
df$Sex <- as.factor(df$Sex)
df$BP <- as.factor(df$BP)
df$Cholesterol <- as.factor(df$Cholesterol)
df$Drug <- as.factor(df$Drug)
# Split data into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(df), 0.7 * nrow(df))
train_data <- df[train_indices, ]
test_data <- df[-train_indices, ]
# 5) Fit a classification and regression model
library(rpart)
tree_model <- rpart(Drug ~ ., data = train_data, method = "class")
# 6) Plot the decision tree for fitted model
library(rpart.plot)
rpart.plot(tree_model, extra = 106)
# 7) Prune the tree by changing the best value
printcp(tree_model)
plotcp(tree_model)
best_cp <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]), "CP"]
pruned_tree <- prune(tree_model, cp = best_cp)
rpart.plot(pruned_tree, extra = 106)
# 8) Observe the results by calculating the misclassification rate or accuracy
# For unpruned tree
tree_pred <- predict(tree_model, test_data, type = "class")
tree_accuracy <- mean(tree_pred == test_data$Drug)
cat("Decision Tree Accuracy:", tree_accuracy, "\n")
# For pruned tree
pruned_pred <- predict(pruned_tree, test_data, type = "class")
pruned_accuracy <- mean(pruned_pred == test_data$Drug)
cat("Pruned Tree Accuracy:", pruned_accuracy, "\n")
# 9) Fit a bagging and random forest model
library(randomForest)
# Bagging (mtry = total number of predictors)
num_predictors <- ncol(df) - 1
bagging_model <- randomForest(Drug ~ ., data = train_data, mtry = num_predictors, ntree = 500)
bagging_pred <- predict(bagging_model, test_data)
bagging_accuracy <- mean(bagging_pred == test_data$Drug)
cat("Bagging Accuracy:", bagging_accuracy, "\n")
# Random Forest (default mtry = sqrt(p) for classification)
rf_model <- randomForest(Drug ~ ., data = train_data, ntree = 500)
rf_pred <- predict(rf_model, test_data)
rf_accuracy <- mean(rf_pred == test_data$Drug)
cat("Random Forest Accuracy:", rf_accuracy, "\n")
print(rf_model)
# 10) Change the value of number of predictors considered for each split
# Try different mtry values
mtry_values <- c(2, 3, 4)
rf_results <- data.frame(mtry = integer(), accuracy = numeric())
for (m in mtry_values) {
rf_temp <- randomForest(Drug ~ ., data = train_data, mtry = m, ntree = 500)
pred_temp <- predict(rf_temp, test_data)
acc_temp <- mean(pred_temp == test_data$Drug)
rf_results <- rbind(rf_results, data.frame(mtry = m, accuracy = acc_temp))
cat("Random Forest with mtry =", m, "Accuracy:", acc_temp, "\n")
}
# 11) Find the best model using parameter tuning
library(caret)
# Define tuning grid
tune_grid <- expand.grid(mtry = c(2, 3, 4, 5))
# Define cross-validation method
control <- trainControl(method = "cv", number = 5)
# Train the model with parameter tuning
set.seed(123)
rf_tuned <- train(Drug ~ ., data = df, method = "rf",
tuneGrid = tune_grid,
trControl = control)
# Print results
print(rf_tuned)
print(rf_tuned$results)
cat("Best mtry value:", rf_tuned$bestTune$mtry, "\n")
# Final model with best parameters
final_model <- randomForest(Drug ~ ., data = train_data,
mtry = rf_tuned$bestTune$mtry,
ntree = 500)
# Evaluate on test data
final_pred <- predict(final_model, test_data)
final_accuracy <- mean(final_pred == test_data$Drug)
cat("Final Model Accuracy:", final_accuracy, "\n")
# Create confusion matrix
conf_matrix <- table(Predicted = final_pred, Actual = test_data$Drug)
print(conf_matrix)
setwd(getwd())
df <- read.csv("drug200.csv")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
setwd(getwd())
df <- read.csv("drug200.csv")
# Display the first few rows
head(df)
set.seed(123)
train_indices <- sample(1:nrow(df), 0.7 * nrow(df))
train_data <- df[train_indices, ]
test_data <- df[-train_indices, ]
cat("Training set size:", nrow(train_data), "rows\n")
cat("Testing set size:", nrow(test_data), "rows\n")
